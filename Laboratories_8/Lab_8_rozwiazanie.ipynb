{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9afa68f-ad65-416e-bcfe-107b105ab3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Zadanie 1\n",
    "\n",
    "Flatten json - wczytaj 10 wybranych atrybutów z pliku brzydki.json (kolumna features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f72887-2f10-4c34-844e-7cb8dc1c5d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+--------------------+----------+------------------+--------------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------+------------------+\n|id_cechy                            |obiekt_docelowy     |typ_zmiany|referencja_zadania|obowiazuje_od       |typ_geometrii|koordynaty                                                                                                                                                 |forma_podstawowa  |status_obiegu|funkcja_podstawowa|\n+------------------------------------+--------------------+----------+------------------+--------------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------+------------------+\n|a6a7e7ff-71d3-e455-0a2c5c6a         |3544354345349395541 |update    |3453654           |2022-08-06T03:29:46Z|Polygon      |[[[445751.0,244970.0,138.984], [446492.041,244970.0,101.861], [446492.041,245088.372,102.604], [445751.0,245088.372,140.312], [445751.0,244970.0,138.984]]]|Named Thoroughfare|Defined      |null              |\n|17e3f1c4-1802-4952-a826-ba148d063716|osgb5000005233652043|delete    |34567535          |2022-08-06T03:29:46Z|LineString   |[[446492.041, 245087.712, 102.616], [446498.463, 245087.71, 103.178], [446503.199, 245088.442, 103.205], [446510.93, 245089.636, 102.862]]                 |Single Carriageway|Defined      |Restricted Access |\n+------------------------------------+--------------------+----------+------------------+--------------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "json_path = \"/FileStore/tables/brzydki.json\"\n",
    "raw_df = spark.read.option(\"multiline\", \"true\").json(json_path)\n",
    "\n",
    "features_df = raw_df.select(explode(col(\"features\")).alias(\"feature\"))\n",
    "\n",
    "selected_df = features_df.select(\n",
    "    col(\"feature.properties.featureId\").alias(\"id_cechy\"),\n",
    "    col(\"feature.properties.toid\").alias(\"obiekt_docelowy\"),\n",
    "    col(\"feature.properties.changeEventType\").alias(\"typ_zmiany\"),\n",
    "    col(\"feature.properties.jobReference\").alias(\"referencja_zadania\"),\n",
    "    col(\"feature.properties.validFromTimestamp\").alias(\"obowiazuje_od\"),\n",
    "    col(\"feature.geometry.type\").alias(\"typ_geometrii\"),\n",
    "    col(\"feature.geometry.coordinates\").alias(\"koordynaty\"),\n",
    "    col(\"feature.properties.baseFormComponent.form\").alias(\"forma_podstawowa\"),\n",
    "    col(\"feature.properties.lifecycleStatusComponent.lifecycleStatus\").alias(\"status_obiegu\"),\n",
    "    col(\"feature.properties.baseFunctionComponents\")[0][\"function\"].alias(\"funkcja_podstawowa\")\n",
    ")\n",
    "\n",
    "selected_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c2105d-aef6-478f-b776-b4f2b0163ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Zadanie 2\n",
    "\n",
    "Poka Yoke\n",
    "\n",
    "Napisz 5 metody, które mogą być użyte w Pipeline tak aby tyły odporne na błędy użytkownika, jak najbardziej „produkcyjnie”. Możesz użyć tego co już stworzyłeś/laś i usprawnij rozwiązanie na bardziej odporne na błędy biorąc pod uwagę dobre praktyki.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95099c80-d12e-498d-9629-ce529613ad64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**`validate_column_ranges(df, ranges)`**\n",
    "\n",
    "Sprawdza, czy wartości liczbowe w kolumnach mieszczą się w określonych granicach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aeec340-f39a-4c46-9167-6755902da5ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validate_column_ranges(df, ranges: dict):\n",
    "    \"\"\"\n",
    "    Check whether numerical values in columns fall within the specified ranges.\n",
    "\n",
    "    :param df: pd.DataFrame\n",
    "    :param ranges: dict – mapping column name to (min, max) allowed values\n",
    "    \"\"\"\n",
    "    for column, (low, high) in ranges.items():\n",
    "        if column in df.columns:\n",
    "            if not df[column].between(low, high).all():\n",
    "                raise ValueError(f\"Column '{column}' has values outside expected range {low}-{high}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415cd1e2-b97a-4706-8638-c4465ee875a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**`require_columns(df, expected_cols)`**\n",
    "\n",
    "Zapewnia obecność niezbędnych kolumn w ramce danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d2ab9d-53ab-4748-8b96-cb76b516a2e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def require_columns(df, expected_cols: list):\n",
    "    \"\"\"\n",
    "    Ensure that all specified columns are present in the DataFrame.\n",
    "\n",
    "    :param df: pd.DataFrame or pyspark.sql.DataFrame\n",
    "    :param expected_cols: list of column names that must exist\n",
    "    \"\"\"\n",
    "    present = df.columns\n",
    "    missing = [c for c in expected_cols if c not in present]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7589683-be19-41b1-b301-5cd54cbf1036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**`rename_columns_safely(df, mapping)`**\n",
    "\n",
    "Bezpiecznie zmienia nazwy kolumn, tylko jeśli istnieją."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1c1dc5-531e-4336-8af1-5a6536a6d273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_columns_safely(df, mapping: dict):\n",
    "    \"\"\"\n",
    "    Rename columns only if all old names are present.\n",
    "\n",
    "    :param df: pd.DataFrame\n",
    "    :param mapping: dict with old -> new column names\n",
    "    :return: DataFrame with renamed columns\n",
    "    \"\"\"\n",
    "    absent = [k for k in mapping.keys() if k not in df.columns]\n",
    "    if absent:\n",
    "        raise KeyError(f\"Cannot rename missing columns: {absent}\")\n",
    "    return df.rename(columns=mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b04c88e-08b7-41d4-90e2-bd77c2a2ef85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**`convert_columns(df, type_schema)`**\n",
    "\n",
    "Konwertuje kolumny do określonych typów, jeśli to możliwe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507218a1-f099-4959-9d4d-acba0ea2f7b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_columns(df, type_schema: dict):\n",
    "    \"\"\"\n",
    "    Try to convert column types to those provided in the schema.\n",
    "\n",
    "    :param df: pd.DataFrame\n",
    "    :param type_schema: dict {column: type}\n",
    "    :return: DataFrame with converted columns\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    for col_name, col_type in type_schema.items():\n",
    "        if col_name in df.columns:\n",
    "            try:\n",
    "                result[col_name] = df[col_name].astype(col_type)\n",
    "            except Exception as e:\n",
    "                raise TypeError(f\"Column '{col_name}' could not be cast to {col_type}: {e}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7215c97d-9355-4082-a45b-b4b6432f3da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**`check_unique_key(df, key_cols)`**\n",
    "\n",
    "Weryfikuje, czy kombinacja kluczy jest unikalna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c724d929-8055-4ac8-96e7-25b17a1ecadd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_unique_key(df, key_cols: list):\n",
    "    \"\"\"\n",
    "    Validate uniqueness of records based on given key columns.\n",
    "\n",
    "    :param df: pd.DataFrame\n",
    "    :param key_cols: list of columns forming a composite key\n",
    "    \"\"\"\n",
    "    if df.duplicated(subset=key_cols).any():\n",
    "        raise ValueError(f\"Duplicate entries found for keys: {key_cols}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab_8_rozwiazanie",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}